{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYTM3oGIj6tD"
      },
      "source": [
        "#GLM Case Study\n",
        "\n",
        "In this tutorial, we will present a detailed case study in the context of auto liability insurance, showcasing a variety of GLM techniques.\n",
        "\n",
        "Let's start by loading the libraries that are going to be helpful. We're going to rely on the statistical learning toolkit ski-cit learn, which provides GLM functionalty but also will be used in the context of other ML models. It is less comfortable to use than some of the other packages. But it is versatile and fast, and therefore one of the most popular predictive modeling toolkits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aL8OZzdpMbDG"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import PoissonRegressor\n",
        "from sklearn.linear_model import GammaRegressor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meSC5JMRkagB"
      },
      "source": [
        "We consider predictive modeling of auto claims, i.e. the overarching challenge is predicting frequencies and/or severities of claims.  We rely on the comprehensive French Motor Third-Part Liability datasets `ferMTPLfreq` and `ferMTPLsev` available within the [package CASdatasets](http://cas.uqam.ca/).\n",
        "\n",
        "Let's take a peak, first at the frequency dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Bglok6-BCHZ"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/danielbauer1979/MSDIA_PredictiveModelingAndMachineLearning.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUQXZq64MbDI"
      },
      "outputs": [],
      "source": [
        "dat_frq1 = pd.read_csv('MSDIA_PredictiveModelingAndMachineLearning/GB886_III_7_freMTPLfreq1.csv')\n",
        "dat_frq2 = pd.read_csv('MSDIA_PredictiveModelingAndMachineLearning/GB886_III_7_freMTPLfreq2.csv')\n",
        "dat_frq = pd.concat([dat_frq1,dat_frq2])\n",
        "dat_frq.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Npw581FNMbDJ"
      },
      "outputs": [],
      "source": [
        "dat_frq.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e__rQGGOEfK7"
      },
      "outputs": [],
      "source": [
        "pd.crosstab(index=dat_frq['ClaimNb'], columns=\"count\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tsQ6OdJqMbDK"
      },
      "outputs": [],
      "source": [
        "pd.crosstab(index=dat_frq['ClaimNb'], columns=\"count\").plot(kind='bar')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQtsK-vWkr5p"
      },
      "source": [
        "So, as expected, multiple claims are rare. The vast majority of cases don't have a claim.\n",
        "\n",
        "Let's look at the severities:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hejS5buBMbDL"
      },
      "outputs": [],
      "source": [
        "dat_sev = pd.read_csv('MSDIA_PredictiveModelingAndMachineLearning/GB886_III_7_freMTPLsev.csv')\n",
        "print(dat_sev.shape)\n",
        "dat_sev.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Usuf8QDMbDM"
      },
      "outputs": [],
      "source": [
        "dat_sev.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1jiPiTElHih"
      },
      "source": [
        "So, again, as expected, we have a few very large claims."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ibmOqUTlMlk"
      },
      "source": [
        "## Merge the Data Sets\n",
        "\n",
        "Since there are multiple claims for each policy, let's summarize the claims to the policy level, so as to allow for an easy merge:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGtRhLmeMbDN"
      },
      "outputs": [],
      "source": [
        "df = dat_sev.groupby('PolicyID', as_index=False).agg({\"ClaimAmount\":\"mean\"})\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNFSHGzclsBf"
      },
      "source": [
        "Now we can simply merge the frequency and the severity sets into our master data set, where we set the `NA` `ClaimAmount` entries to zero where we don't have claims:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pga7xTJlMbDO"
      },
      "outputs": [],
      "source": [
        "dat = pd.merge(dat_frq, dat_sev.groupby('PolicyID', as_index=False).agg({\"ClaimAmount\":\"mean\"}),how='left')\n",
        "dat = dat.fillna(0)\n",
        "dat.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuHNeCFBlw9o"
      },
      "source": [
        "## Building a GLM\n",
        "\n",
        "As previously, we need to put the categorical variables to dummies. For that, we separate the dummies and the numerical variables, make the dummies, and then concatenate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGomSaWpG_bl"
      },
      "outputs": [],
      "source": [
        "dummies = pd.get_dummies(dat[dat.columns[[4,7,8,9]]],drop_first=True)\n",
        "dat = dat.drop(dat.columns[[0, 1, 4, 7, 8, 9]], axis=1)\n",
        "dat = pd.concat([dat,dummies], axis=1)\n",
        "dat.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPlQ7vLNmV9D"
      },
      "source": [
        "Let's do some visualizations of some of the variables:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLk-Pkk6MbDP"
      },
      "outputs": [],
      "source": [
        "plt.hist(dat['Exposure'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vR7Moc-3MbDQ"
      },
      "outputs": [],
      "source": [
        "plt.hist(dat['DriverAge'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRrdK8pXmbAx"
      },
      "source": [
        "...likely what we expected..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06d-HUe8MbDR"
      },
      "outputs": [],
      "source": [
        "plt.hist(dat['CarAge'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcf3qxJNmflv"
      },
      "source": [
        "...also no surprises...\n",
        "\n",
        "Let's look at population density, that's a more interesting variable:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdrgJHVNMbDS"
      },
      "outputs": [],
      "source": [
        "plt.hist(dat['Density'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdnMknGMmnMo"
      },
      "source": [
        "So it looks like very small and a few very high densities. Let's go to log-scale."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-q1ivoVPMbDS"
      },
      "outputs": [],
      "source": [
        "dat.loc[:, 'Density'] = np.log(dat['Density'])\n",
        "plt.hist(dat['Density'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V01SE4mvm3LO"
      },
      "source": [
        "Let's also check out our targets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1C8k2NeuMbDT"
      },
      "outputs": [],
      "source": [
        "dat.loc[dat['ClaimAmount']>0,'ClaimAmount'].quantile([.9, .95, .99, .999])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tp2_MWppnAkp"
      },
      "source": [
        "It is possible that the few very large claims will cause trouble, so let's cut off at 50K:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WqSAP0adMbDU"
      },
      "outputs": [],
      "source": [
        "dat['ClaimAmount'][dat['ClaimAmount']>50000] = 50000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhfVK2DPnN7U"
      },
      "source": [
        "Let's define two target variables: the number of claims and the loss amount"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TO9-A2vSIWYE"
      },
      "outputs": [],
      "source": [
        "y_freq = dat['ClaimNb']\n",
        "y_sev = dat['ClaimAmount']\n",
        "X = dat.drop(columns = ['ClaimNb','ClaimAmount'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JN1y97tDne85"
      },
      "source": [
        "And let's model frequencies via a Poisson Regression:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjSS_F-lKOxO"
      },
      "outputs": [],
      "source": [
        "freqmodel = PoissonRegressor()\n",
        "freqmodel.fit(X,y_freq)\n",
        "preds_freq = freqmodel.predict(X)\n",
        "np.corrcoef(preds_freq,y_freq)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQQVog1fnrre"
      },
      "source": [
        "The correlation is fairly low, but when we look at a scatter plot..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WfqWyDw-MniH"
      },
      "outputs": [],
      "source": [
        "plt.scatter(y_freq,preds_freq)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAqmuQHrn7qq"
      },
      "source": [
        "it does seem like that the claims with higher frequencies have higher predictions, though they are still close to zero. Accurately predicting car accidents is just very difficult."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnuEZkfLoRTN"
      },
      "source": [
        "Let's looks at claims:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h18_spWDMbDW"
      },
      "outputs": [],
      "source": [
        "plt.hist(dat['ClaimAmount'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVZLmKEnoXY7"
      },
      "source": [
        "and non-zero claims on a log-scale."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7AzVYDaeMbDW"
      },
      "outputs": [],
      "source": [
        "plt.hist(np.log(dat.loc[dat['ClaimAmount']>0,'ClaimAmount']))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcFGWycAob2g"
      },
      "source": [
        "Let's run a Gamma regression for the severities:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7QVwGKbcNAT8"
      },
      "outputs": [],
      "source": [
        "X_sev = dat.loc[dat['ClaimAmount']>0,:]\n",
        "y_sev = X_sev['ClaimAmount']\n",
        "X_sev = X_sev.drop(columns = ['ClaimNb','ClaimAmount'])\n",
        "sevmodel = GammaRegressor()\n",
        "sevmodel.fit(X_sev,y_sev)\n",
        "plt.scatter(y_sev,sevmodel.predict(X_sev))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3swOb71oplI"
      },
      "source": [
        "So we're kind-of catching the trend.\n",
        "\n",
        "Now we can fuse together by muliplying predicted frequencies and severities:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNfxc8OjP06W"
      },
      "outputs": [],
      "source": [
        "preds_sev = sevmodel.predict(X)\n",
        "preds_tot = preds_freq * preds_sev"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGRhWTucQVNh"
      },
      "outputs": [],
      "source": [
        "plt.scatter(y_freq * dat['ClaimAmount'],preds_tot)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
